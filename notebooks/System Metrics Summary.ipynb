{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling system data and metrics from Qumulo\n",
    "#### *This is an unsupported and highly experimental approach for pulling metrics data from a Qumulo cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "# python + ssl on MacOSX is rather noisy against dev clusters\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_KEYS = OrderedDict([\n",
    "    ('ad_base_ldap_client--request_latency_calls', {'name':'active_directory~op_count'}),\n",
    "    ('ad_base_ldap_client--request_latency_total_time_usec', {'name':'active_directory~msec', 'divide':1000}),\n",
    "\n",
    "    ('trans_stats-name:fs_create_file-calls', {'name':'file_create~op_count'}),\n",
    "    ('trans_stats-name:fs_create_file-time_sum', {'name':'file_create~msec', 'divide':1000}),\n",
    "    ('trans_stats-name:fs_rename-calls', {'name':'file_rename~op_count'}),\n",
    "    ('trans_stats-name:fs_rename-time_sum', {'name':'file_rename~msec', 'divide':1000}),\n",
    "    ('trans_stats-name:fs_unlink-calls', {'name':'file_delete~op_count'}),\n",
    "    ('trans_stats-name:fs_unlink-time_sum', {'name':'file_delete~msec', 'divide':1000}),\n",
    "\n",
    "    ('trans_stats-name:fs_read-data_read_hdd_misses', {'name':'file_read_cache_miss~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats-name:fs_read-data_read_ssd_misses', {'name':'file_read_cache_miss~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats-name:fs_read-data_read_disk_cache_hits', {'name':'file_read_from_ram~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats-name:fs_read-data_read_trans_cache_hits', {'name':'file_read_from_ram~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats-name:fs_read-data_read_on_hdd', {'name':'file_read_from_hdd~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats-name:fs_read-data_read_on_ssd', {'name':'file_read_from_ssd~bytes', 'divide':1.0/4096}),\n",
    "\n",
    "    ('global_perf_counters--snapshots:garbage_collector:blocks_deleted', {'name':'snapshot_deleted_data~bytes', 'divide':1.0/4096}),\n",
    "    ('global_perf_counters--snapshots:garbage_collector:inodes_deleted', {'name':'snapshot_deleted_inodes~count'}),\n",
    "    ('global_perf_counters--snapshots:garbage_collector:snapshots_deleted', {'name':'snapshot_deleted_snapshots~count'}),\n",
    "    ('global_perf_counters--fs:statistics:deferred_bytes_deleted', {'name':'deleted_deferred~bytes'}),\n",
    "    ('global_perf_counters--fs:statistics:snapshot_bytes_deleted', {'name':'deleted_snapshot~bytes'}),\n",
    "    ('global_perf_counters--tree_delete:adapter:unlinked_count', {'name':'tree_delete_files_deleted~count'}),\n",
    "    ('global_perf_counters--tree_delete:deleter:tree_delete_count', {'name':'tree_delete~op_count'}),\n",
    "    ('global_perf_counters--fs:prefetcher:contiguous_policy_prefetch_waste', {'name':'prefetch_waste~bytes'}),\n",
    "    ('global_perf_counters--fs:prefetcher:contiguous_policy_prefetched', {'name':'prefetch_success~bytes'}),\n",
    "    ('global_perf_counters--fs:prefetcher:contiguous_policy_read_not_predicted', {'name':'prefetch_missed~bytes'}),\n",
    "    ('global_perf_counters--fs:prefetcher:contiguous_policy_wasted_lookups', {'name':'prefetch_file_next_waste~count'}),\n",
    "    ('global_perf_counters--fs:prefetcher:next_file_confirmed', {'name':'prefetch_file_next_success~count'}),\n",
    "\n",
    "    ('public_audit_logger--bytes_sent', {'name':'audit_sent~bytes'}),\n",
    "    ('public_audit_logger--messages_buffered', {'name':'audit_buffered~op_count'}),\n",
    "    ('public_audit_logger--messages_dropped_due_to_full_buffer', {'name':'audit_dropped~op_count'}),\n",
    "    ('public_audit_logger--messages_dropped_due_to_full_socket', {'name':'audit_dropped~op_count'}),\n",
    "    ('public_audit_logger--messages_sent', {'name':'audit_sent~op_count'}),\n",
    "    ('audit_log_deduper--duplicate_operations_seen', {'name':'audit_dedup~op_count'}),\n",
    "])\n",
    "\n",
    "RX_KEYS = OrderedDict([\n",
    "    ('trans_stats.*-meta_read_on_hdd', {'name':'metadata_read_from_hdd~bytes', 'divide':1.0/4096}),\n",
    "    ('trans_stats.*-meta_read_on_ssd', {'name':'metadata_read_from_ssd~bytes', 'divide':1.0/4096}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SPINNING_DISK&user:DEFAULT_CLASS-bytes_written', {'name': 'drive_hdd_write~bytes'}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SSD&user:DEFAULT_CLASS-bytes_written', {'name': 'drive_ssd_write~bytes'}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SPINNING_DISK&user:DEFAULT_CLASS-bytes_read', {'name': 'drive_hdd_read~bytes'}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SSD&user:DEFAULT_CLASS-bytes_read', {'name': 'drive_ssd_read~bytes'}),\n",
    "\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SPINNING_DISK&user:DEFAULT_CLASS-.*latency_calls', {'name': 'drive_hdd~op_count'}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SSD&user:DEFAULT_CLASS-.*latency_calls', {'name': 'drive_ssd~op_count'}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SPINNING_DISK&user:DEFAULT_CLASS-.*latency_total_time_usec', {'name': 'drive_hdd~msec', 'divide':1000}),\n",
    "    ('elevator_user.*type:DEVICE_MEDIA_TYPE_SSD&user:DEFAULT_CLASS-.*latency_total_time_usec', {'name': 'drive_ssd~msec', 'divide':1000}),\n",
    "\n",
    "    ('network_interface-interface:eth.*&state:active&network:both-dropped_packets', {'name': 'network_dropped_packets_node%s~op_count'}),\n",
    "    ('network_interface-interface:eth.*&state:active&network:both-frame_errors', {'name': 'network_frame_errors_node%s~op_count'}),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QumuloMetrics:\n",
    "    def __init__(self, host, username, password):\n",
    "        self.metrics = {}\n",
    "        self.ROOT_URL = 'https://' + host + ':8000'\n",
    "        self.URLS = {\n",
    "            'login': '/v1/session/login',\n",
    "            'drive_status': '/v1/cluster/slots/',\n",
    "            'node_status': '/v1/cluster/nodes/',\n",
    "            'node_hw': '/v1/cluster/nodes/chassis/',\n",
    "            'capacity': '/v1/file-system',\n",
    "            'root_aggregate': '/v1/files/%2F/aggregates/?max-entries=0',\n",
    "            'metrics': '/v1/metrics/?node-id=%s',\n",
    "            'network_status': '/v2/network/interfaces/%s/status/',\n",
    "            'protocol_connections': '/v2/network/connections/',\n",
    "            'restriper_status': '/v1/cluster/restriper/status',\n",
    "            'snapshots': '/v2/snapshots/status/',\n",
    "            'replication_source': '/v2/replication/source-relationships/status/',\n",
    "            'replication_target': '/v2/replication/target-relationships/status/',\n",
    "            'quotas_status': '/v1/files/quotas/status/?limit=2',\n",
    "            'smb_share_status': '/v2/smb/shares/',\n",
    "            'nfs_export_status': '/v2/nfs/exports/',\n",
    "            'roles': '/v1/auth/roles/',\n",
    "            'role_members': '/v1/auth/roles/%s/members',\n",
    "            'snapshot_policies': '/v1/snapshots/policies/status/',\n",
    "        }\n",
    "        for k, url in self.URLS.items():\n",
    "            self.URLS[k] = self.ROOT_URL + url\n",
    "        self.header = {'content-type': 'application/json'}\n",
    "        resp = requests.post(self.URLS['login'], \n",
    "                          data=json.dumps({'username': username, 'password': password}), \n",
    "                          headers=self.header, \n",
    "                          verify=False)\n",
    "        resp_data = json.loads(resp.text)\n",
    "        self.header['Authorization'] = 'Bearer ' + resp_data['bearer_token']\n",
    "        resp = requests.get(self.URLS['node_status'], headers=self.header, verify=False)\n",
    "        self.nodes = json.loads(resp.text)\n",
    "        \n",
    "    def add_metric(self, k, v=0, replace=False):\n",
    "        if k not in self.metrics or replace:\n",
    "            self.metrics[k] = v\n",
    "        else:\n",
    "            self.metrics[k] += v\n",
    "\n",
    "    def get_metrics_inventory(self):\n",
    "        data = {}\n",
    "        for node in self.nodes:\n",
    "            resp = requests.get(URLS[\"metrics\"] % node['id'], headers=header, verify=False)\n",
    "            node_metrics = json.loads(resp.text)\n",
    "            for node_metric in node_metrics:\n",
    "                for name, val in node_metric['fields'].items():\n",
    "                    measure = node_metric['measurement']\n",
    "                    tags = '&'.join([\"%s:%s\" % (k, v) for k, v in node_metric['tags'].items()])\n",
    "                    key = \"%s\\t%s\\t%s\\t%s-%s-%s\" % (measure, tags, name, measure, tags, name)\n",
    "                    if key not in data:\n",
    "                        data[key] = float(val)\n",
    "                    else:\n",
    "                        data[key] += float(val)\n",
    "        return data\n",
    "\n",
    "            \n",
    "    def get_metrics(self, full_keys, rx_keys):\n",
    "        self.metrics = {}\n",
    "        for node in self.nodes:\n",
    "            resp = requests.get(self.URLS[\"metrics\"] % node['id'], headers=self.header, verify=False)\n",
    "            node_metrics = json.loads(resp.text)\n",
    "            for node_metric in node_metrics:\n",
    "                full_key = node_metric['measurement'] + \"-\" + '&'.join([\"%s:%s\" % (k, v) for k, v in node_metric['tags'].items()])\n",
    "                fields = node_metric['fields']\n",
    "                if node_metric['measurement'] == 'protocol_op':\n",
    "                    op = node_metric['tags']['op_name']\n",
    "                    protocol = op[0:3].lower()\n",
    "                    op_short = 'write' if re.match(\".*(WRITE|SET|CREATE).*\", op) else 'read'\n",
    "                    op_type = 'metadata'\n",
    "                    if int(fields['total_bytes']) > 0:\n",
    "                        self.add_metric('file_%s_data~msec' % (op_short), int(fields['total_latency_total_time_usec'])/1000.0)\n",
    "                        self.add_metric('file_%s_data~bytes' % op_short, int(fields['total_bytes']))\n",
    "                        self.add_metric('file_%s_data~op_count' % (op_short), int(fields['total_latency_calls']))\n",
    "                        op_type = 'data'\n",
    "                    self.add_metric('file_%s_%s_%s~op_count' % (op_short, op_type, protocol), int(fields['total_latency_calls']))\n",
    "                    self.add_metric('file_%s_%s_%s~msec' % (op_short, op_type, protocol), int(fields['total_latency_total_time_usec'])/1000.0)\n",
    "                    if op_type == 'data':\n",
    "                        self.add_metric('file_%s_%s_%s~bytes' % (op_short, op_type, protocol), int(fields['total_bytes']))\n",
    "                        self.add_metric('file_total_%s_node%s~bytes' % (op_short, \"%03d\" % node['id']), int(fields['total_bytes']))\n",
    "\n",
    "                    self.add_metric('file_total_node%s~msec' % (\"%03d\" % node['id']), int(fields['total_latency_total_time_usec'])/1000.0)\n",
    "                    self.add_metric('file_total_node%s~op_count' % (\"%03d\" % node['id']), int(fields['total_latency_calls']))\n",
    "\n",
    "                    for name, val in fields.items():\n",
    "                        if 'io_size_bucket_' in name:\n",
    "                            bytes_num = 1048576\n",
    "                            if int(name.split('_')[3]) <= 4096:\n",
    "                                bytes_num = 4096\n",
    "                            elif int(name.split('_')[3]) <= 65536:\n",
    "                                bytes_num = 65536\n",
    "                            self.add_metric('file_%s_data_%07dbytes~op_count' % (op_short, bytes_num), int(val))\n",
    "\n",
    "                for name, val in fields.items():\n",
    "                    k = full_key + \"-\" + name\n",
    "                    if k in full_keys:\n",
    "                        kd = full_keys[k]\n",
    "                        self.add_metric(kd['name'], int(val) / (1 if 'divide' not in kd else kd['divide']))\n",
    "                    else:\n",
    "                        for rx, kd in rx_keys.items():\n",
    "                            if re.match(rx, k):\n",
    "                                if 'node' in kd['name']:\n",
    "                                    self.add_metric(kd['name'] % (\"%03d\" % node['id']), int(val) / (1 if 'divide' not in kd else kd['divide']))\n",
    "                                else:\n",
    "                                    self.add_metric(kd['name'], int(val) / (1 if 'divide' not in kd else kd['divide']))\n",
    "\n",
    "        ###############   network status   ###############\n",
    "        resp = requests.get(self.URLS['network_status'] % 1, headers=self.header, verify=False)\n",
    "        for node in json.loads(resp.text):\n",
    "            self.add_metric('network_mbit_node%03d~speed' % int(node[\"node_id\"]), \n",
    "                            node['interface_details']['speed'], True)\n",
    "            self.add_metric('network_recv_node%03d~bytes' % int(node[\"node_id\"]), \n",
    "                            node['interface_details']['bytes_received'], True)\n",
    "            self.add_metric('network_sent_node%03d~bytes' % int(node[\"node_id\"]), \n",
    "                            node['interface_details']['bytes_sent'], True)\n",
    "\n",
    "        ###############   restriper status   ###############\n",
    "        resp = requests.get(self.URLS['restriper_status'], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('restriper_elapsed_time~seconds' % node, \n",
    "                        d['elapsed_seconds'], True)\n",
    "        self.add_metric('restriper_time_left~seconds' % node, \n",
    "                        d['estimated_seconds_left'], True)\n",
    "\n",
    "        ###############   drive status   ###############\n",
    "        resp = requests.get(self.URLS['drive_status'], headers=self.header, verify=False)\n",
    "        for d in json.loads(resp.text):\n",
    "            self.add_metric('drive_%s_healthy~count' % d['disk_type'].lower(), 1 if d['state'] == 'healthy' else 0)\n",
    "            self.add_metric('drive_%s_unhealthy~count' % d['disk_type'].lower(), 1 if d['state'] != 'healthy' else 0)\n",
    "\n",
    "        ###############   node status   ###############\n",
    "        resp = requests.get(self.URLS['node_status'], headers=self.header, verify=False)\n",
    "        for d in json.loads(resp.text):\n",
    "            self.add_metric('node_online~count', 1 if d['node_status'] == 'online' else 0)\n",
    "            self.add_metric('node_unhealthy~count', 1 if d['node_status'] != 'online' else 0)\n",
    "\n",
    "        ###############   psu status   ###############\n",
    "        resp = requests.get(self.URLS['node_hw'], headers=self.header, verify=False)\n",
    "        for n in json.loads(resp.text):\n",
    "            for d in n['psu_statuses']:\n",
    "                self.add_metric('psu_healthy~count' , 1 if d['state'] == 'GOOD' else 0)\n",
    "                self.add_metric('psu_unhealthy~count' , 1 if d['state'] != 'GOOD' else 0)\n",
    "\n",
    "        ###############   capacity    ###############\n",
    "        resp = requests.get(self.URLS['root_aggregate'], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('capacity_directories~count', int(d['total_directories']))\n",
    "        self.add_metric('capacity_files~count', int(d['total_files']))\n",
    "        self.add_metric('capacity_other~count', int(d['total_other_objects']) + int(d['total_symlinks']))\n",
    "        self.add_metric('capacity_streams~count', int(d['total_named_streams']))\n",
    "        \n",
    "        resp = requests.get(self.URLS['capacity'], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('capacity_free~bytes', int(d['free_size_bytes']))\n",
    "        self.add_metric('capacity_snapshots~bytes', int(d['snapshot_size_bytes']))\n",
    "        self.add_metric('capacity_usable~bytes', int(d['total_size_bytes']))\n",
    "        self.add_metric('capacity_used~bytes', int(d['total_size_bytes']) - int(d['free_size_bytes']))\n",
    "\n",
    "        ###############   snapshots   ###############\n",
    "        resp = requests.get(self.URLS['snapshots'], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('snapshot_root~count', 0)\n",
    "        for snap in d['entries']:\n",
    "            if snap['source_file_path'] == '/':\n",
    "                self.add_metric('snapshot_root~count', 1)\n",
    "            if snap['expiration'] == '':\n",
    "                self.add_metric('snapshot_no_expiration~count', 1)\n",
    "            self.add_metric('snapshot_total~count', 1)\n",
    "        resp = requests.get(self.URLS[\"snapshot_policies\"], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        for snap in d['entries']:\n",
    "            self.add_metric('snapshot_policies_total~count', 1)\n",
    "            if snap['enabled']:\n",
    "                self.add_metric('snapshot_policies_enabled_total~count', 1)\n",
    "\n",
    "        ###############   replication   ###############            \n",
    "        for repl_type in ['source', 'target']:\n",
    "            resp = requests.get(self.URLS['replication_%s' % repl_type], headers=self.header, verify=False)\n",
    "            dd = json.loads(resp.text)\n",
    "            for d in dd:\n",
    "                self.add_metric('replication_%s_relationship~count' % repl_type, 1)\n",
    "                if d['state'] != 'ESTABLISHED':\n",
    "                    self.add_metric('replication_%s_relationship_unestablished~count' % repl_type, 1)\n",
    "                if d['job_state'] != 'REPLICATION_NOT_RUNNING':\n",
    "                    self.add_metric('replication_%s_relationship_active~count' % repl_type, 1)\n",
    "                    if d['replication_job_status'] is not None:\n",
    "                        self.add_metric('replication_%s_active_files_remaining~count' % repl_type, \n",
    "                                        int(d['replication_job_status']['files_remaining']))\n",
    "                        self.add_metric('replication_%s_active_throughput~bytepersec' % repl_type, \n",
    "                                        int(d['replication_job_status']['throughput_current']))\n",
    "\n",
    "        ###############   quotas   ###############        \n",
    "        next_url = self.URLS[\"quotas_status\"]\n",
    "        while next_url != '':\n",
    "            resp = requests.get(next_url, headers=self.header, verify=False)\n",
    "            d = json.loads(resp.text)\n",
    "            if 'quotas' not in d:\n",
    "                next_url = ''\n",
    "                break\n",
    "            for q in d['quotas']:\n",
    "                self.add_metric('quotas~count', 1)\n",
    "                if int(q['capacity_usage']) >= q['limit']:\n",
    "                    self.add_metric('quotas_exceeded~count', 1)\n",
    "                if int(q['capacity_usage']) >= q['limit']*95:\n",
    "                    self.add_metric('quotas_exceeded_95~count', 1)\n",
    "            next_url = self.ROOT_URL + d['paging']['next']\n",
    "\n",
    "        ###############   smb shares   ###############        \n",
    "        resp = requests.get(self.URLS[\"smb_share_status\"], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        for share in d:\n",
    "            self.add_metric('smb_share~count', 1)\n",
    "            if share['require_encryption']:\n",
    "                self.add_metric('smb_share_encrypted~count', 1)\n",
    "            writable = False\n",
    "            for p in share['permissions']:\n",
    "                if 'WRITE' in p['rights']:\n",
    "                    writable = True\n",
    "            if not writable:\n",
    "                self.add_metric('smb_share_not_writable~count', 1)\n",
    "\n",
    "        ###############   nfs exports   ###############        \n",
    "        resp = requests.get(self.URLS[\"nfs_export_status\"], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        for share in d:\n",
    "            self.add_metric('nfs_exports~count', 1)\n",
    "            if share['restrictions'][0]['read_only']:\n",
    "                self.add_metric('nfs_exports_read_only~count', 1)\n",
    "            if len(share['restrictions'][0]['host_restrictions']) > 0:\n",
    "                self.add_metric('nfs_exports_has_host_restrictions~count', 1)\n",
    "            if len(share['restrictions']) > 1:\n",
    "                self.add_metric('nfs_exports_multiple_restrictions~count', 1)\n",
    "\n",
    "        ###############   roles   ###############        \n",
    "        resp = requests.get(self.URLS[\"roles\"], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('roles_total~count', len(d))\n",
    "        resp = requests.get(self.URLS[\"role_members\"] % 'Administrators', headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        self.add_metric('roles_members_administrators~count', len(d['members']))\n",
    "\n",
    "        ###############   protocol_connections   ###############        \n",
    "        resp = requests.get(self.URLS[\"protocol_connections\"], headers=self.header, verify=False)\n",
    "        d = json.loads(resp.text)\n",
    "        for cc in d:\n",
    "            for c in cc['connections']:\n",
    "                self.add_metric('protocol_connection~count', 1)\n",
    "                self.add_metric('protocol_connection_%s~count' % c['type'][-3:].lower() , 1)\n",
    "\n",
    "        return self.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creds = [\n",
    "        ['test.test.com', 'admin', '****'],\n",
    "        ]\n",
    "\n",
    "fw = open(\"metrics-summary.txt\", \"w\")\n",
    "for cred in creds:\n",
    "    qm = QumuloMetrics(cred[0], cred[1], cred[2])\n",
    "    m0 = qm.get_metrics(FULL_KEYS, RX_KEYS)\n",
    "    time.sleep(55)\n",
    "    m1 = qm.get_metrics(FULL_KEYS, RX_KEYS).items()    \n",
    "    for k, v in sorted(m1):\n",
    "        fw.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (cred[0], k, int(v), int(m0[k]), int(v)-int(m0[k]) ))\n",
    "fw.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
